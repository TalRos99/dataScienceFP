# Step 1:
How to get the details from all these diaries? 
I started with a naive approach. Trying to load 10K of diaries using the website UI wasn't successful. So I carefully examined the query that was sent when more diaries were added. Using Postman (API requests tool), I discovered that the query has a limit of 2K diaries and a starting point and count of how many rows the server should return. Added to it is the fact that I can always be blocked from the website side.
With those conclusions I decided to download the entire HTML pages that the query returned with 2K chunks, using it I scraped 10K links of grow diaries into my first database# Step 2:
Once I have the links for my desired database, I need to start collecting raw data from each diary. As the first step toward real data, my decision was to include the following as features: 
diary_name bud_wet_weight
Using likes, comments, and views may increase/decrease the reliability of the data point.
The method used to scrape the data is XPATH. It has conditional filtering and I found it the perfect way to reach every selector I needed.
Highly recommend: https://devhints.io/xpath for understanding and creating XPath queries
Connect all files to one CSV file: