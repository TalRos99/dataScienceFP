# Step 1:
How to get the details from all these diaries? 
I started with a naive approach, trying to load 10K of diaries using the website UI wasn't successful. So I looked deeply into the query made each time more diaries are added. Using Postman (API requests tool), I realized that the query has a limit of 2K diaries and a starting point and count of how many rows the server should retrieve to the user. Adding on top of it is the fact that I can always be blocked from the website side.
With those conclusions I decided to download the entire HTML pages that the query returns with 2K chunks, using it I scraped 10K links of grow diaries into my first database.# Step 2:
Now, when having the links for my desired database, I need to start collecting the raw data from each diary. As the first action toward the real data, my decision was to include the following as features:
diary_name bud_wet_weight
Using likes, comments, and views may help to increase/decrease the reliability of the data point.
The method used to scrape the data is XPATH, it have conditional filtering and I found it as the perfect way to reach every selector I needed.
Highly recommand on: https://devhints.io/xpath for understanding and creating xpath queries

url_df = pd.read_csv("Diary Links.csv")Connecting all files to one csv file:
