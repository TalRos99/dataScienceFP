{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def click_btn(clickable):\n",
    "    for btn in clickable:\n",
    "        try:\n",
    "            btn.click()\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project data\n",
    "The entire dataset I'm using in this project is scraped from https://growdiaries.com/, as one of the significant weed-growing communities in the world, it holds a large amount of data. Each row represents a data point, each column represents one of the diary features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1:\n",
    "How to get the details from all these diaries? <br>\n",
    "I started with a naive approach, trying to load 10K of diaries using the website UI wasn't successful. So I looked deeply into the query made each time more diaries are added. Using Postman (API requests tool), I realized that the query has a limit of 2K diaries and a starting point and count of how many rows the server should retrieve to the user. Adding on top of it is the fact that I can always be blocked from the website side.\n",
    "With those conclusions I decided to download the entire HTML pages that the query returns with 2K chunks, using it I scraped 10K links of grow diaries into my first database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# startpoint = 0\n",
    "# count = 2000\n",
    "# prefix_url = \"https://growdiaries.com/explore\"\n",
    "\n",
    "# service = Service('./chromedriver.exe')\n",
    "# op = webdriver.ChromeOptions()\n",
    "# # op.add_argument('--headless')\n",
    "# elements = []\n",
    "# url_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver = webdriver.Chrome(service=service, options=op)\n",
    "# while startpoint < 10000:\n",
    "#     query = f\"?action=loadpage&category=all&tags=harvested&start={startpoint}&count={count}&undefined=&ajax=true&ust=9c824c5a63187dd5f579502e26ac3594c3dbb8752fd954432a4d6e1b0ff2e41f8654e9f3\"\n",
    "#     final_url = prefix_url + query\n",
    "#     driver.get(final_url)\n",
    "#     elements= driver.find_elements(By.XPATH, \"//a[@class='name']\")\n",
    "#     startpoint += count\n",
    "#     for element in elements:\n",
    "#         temp_df = pd.DataFrame({\"DiaryName\": [element.text], \"Url\": [element.get_attribute('href')]})\n",
    "#         url_df = pd.concat([url_df, temp_df], ignore_index=True)\n",
    "    \n",
    "# driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url_df.to_csv(\"Diary Links.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2:\n",
    "Now, when having the links for my desired database, I need to start collecting the raw data from each diary. As the first action toward the real data, my decision was to include the following as features:\n",
    "diary_name<br> strain<br> strains_company<br> light_watt<br> nutrients<br> watering<br> soil<br> germination<br> grow_techniques<br>grow_room_size<br> weeks_to_harvest<br> num_of_plants<br> likes<br>comments<br> views<br> bud_dry_weight<br> bud_wet_weight\n",
    "\n",
    "Using likes, comments, and views may help to increase/decrease the reliability of the data point.<br>\n",
    "The method used to scrape the data is XPATH, it have conditional filtering and I found it as the perfect way to reach every selector I needed.\n",
    "\n",
    "Highly recommand on: https://devhints.io/xpath for understanding and creating xpath queries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note: To make it esaier, I saved each time 1K of data in a file</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_df = pd.read_csv(\"Diary Links.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver = webdriver.Chrome(service=service, options=op)\n",
    "# count = 2000\n",
    "# const = 1000\n",
    "# while count < 10000:\n",
    "#     det_df = pd.DataFrame()\n",
    "#     for j, url in enumerate(url_df[\"Url\"][count:count+const]):\n",
    "#         driver.get(url)\n",
    "#         driver.maximize_window()\n",
    "#         clickable = driver.find_elements(By.XPATH, \"//div[@class='btn_less']\")\n",
    "#         click_btn(clickable)\n",
    "#         report_items = driver.find_elements(By.XPATH, \"//div[contains(@class,'report_items')]//*[@class='info']\")\n",
    "#         outcome = driver.find_elements(By.XPATH, \"//div[contains(@class,'parameters_item')]\")\n",
    "#         likes = driver.find_element(By.XPATH, \"//div[@class='report_statistic']//div[.//@class='icon-leaf-like']\")\n",
    "#         comments = driver.find_element(By.XPATH, \"//div[@class='report_statistic']//div[.//@class='icon comment']\")\n",
    "#         views = driver.find_element(By.XPATH, \"//div[@class='report_statistic']//div[.//@class='icon eye']\")\n",
    "#         d = {\"diary_name\": url_df['DiaryName'].loc[count+j], \"strain\": \"\", \"strains_company\":\"\", \"light_watt\": \"\", \"nutrients\": \"\", \"watering\": \"\", \"soil\": \"\", \"germination\": \"\", 'grow_techniques': \"\", \"grow_room_size\": \"\",\n",
    "#         \"weeks_to_harvest\": \"\", \"num_of_plants\": \"\",\"likes\": likes.text, \"comments\": comments.text,\"views\": views.text,\"bud_dry_weight\": \"\", \"bud_wet_weight\": \"\"}\n",
    "\n",
    "        # for i, item in enumerate(report_items):\n",
    "        #     try:\n",
    "        #         text = item.text\n",
    "        #         texts = text.split(\"\\n\")\n",
    "        #         if i == 0:\n",
    "        #             d[\"strain\"] = texts[0]\n",
    "        #             d['strains_company'] = texts[1]\n",
    "        #         elif \"LED\" in text:\n",
    "        #             d['light_watt'] += text.replace(\"\\n\", \" \") + \",\"\n",
    "        #         elif \"Nutrients\" in text:\n",
    "        #             d['nutrients'] += texts[0] +\",\"\n",
    "        #         elif \"Watering\" in text:\n",
    "        #             d['watering'] = texts[0]\n",
    "        #         elif \"Soil\" in text or \"Grow\" in text:\n",
    "        #             d['soil']+= text.replace(\"\\n\", \" \")+\",\"\n",
    "        #         elif \"Germination\" in text:\n",
    "        #             d['germination'] = texts[0]\n",
    "        #         elif text.find(\"Week\") > 0 and len(texts[0]) > 1:\n",
    "        #             d['grow_techniques'] += texts[0] + \",\"\n",
    "        #         elif text.find(\"Tent\") > 0:\n",
    "        #             d['grow_room_size'] = texts[0]\n",
    "        #     except:\n",
    "        #         pass\n",
    "        # for i, details in enumerate(outcome):\n",
    "        #     try:\n",
    "        #         text = details.text\n",
    "        #         texts = text.split(\"\\n\")\n",
    "        #         if i == 0:\n",
    "        #             d[\"weeks_to_harvest\"] = texts[1]\n",
    "        #         elif \"BUD WET WEIGHT\" in text:\n",
    "        #             d['bud_wet_weight']= texts[1]\n",
    "        #         elif \"BUD DRY WEIGHT\" in text:\n",
    "        #             d['bud_dry_weight']= texts[1]\n",
    "        #         elif \"NUMBER OF PLANTS HARVESTED\" in text:\n",
    "        #             d['num_of_plants'] = texts[1].split(\" \")[0]\n",
    "        #         elif \"TOTAL LIGHT POWER\" in text:\n",
    "        #             d['light_watt'] = texts[1]\n",
    "        #         elif \"GROW ROOM\" in text:\n",
    "        #             d['grow_room_size']= texts[1]\n",
    "        #     except:\n",
    "        #         pass\n",
    "        # det_df = pd.concat([det_df,pd.DataFrame(d, index=[0])], ignore_index=True)\n",
    "    # det_df.to_csv(f\"Data_{count}_{count+const}.csv\", index=False)\n",
    "    # count += const\n",
    "\n",
    "# driver.quit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connecting all files to one csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# const = 1000\n",
    "# full_df = pd.DataFrame()\n",
    "# while count < 10000:\n",
    "#     full_df = pd.concat([full_df, pd.read_csv(f\"Data_{count}_{count+const}.csv\")])\n",
    "#     count += const\n",
    "# full_df.to_csv(\"GrowDiariesRowData.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial analysis and data refinement: \n",
    "<small>Nulls: np.nan/NaN/None/NA</small><br>\n",
    "Strating with testing which one of the dry/wet bud weight have less null values and set it as the target variable. <br>\n",
    "After, I was looking for the number of null values in each feature nd wrote it into a dictionery, that's the way to find out the features contains null and how many."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.read_csv(\"GrowDiariesRowData.csv\")\n",
    "full_df['bud_dry_weight'].isna().sum(), full_df['bud_wet_weight'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = full_df.drop(\"bud_dry_weight\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing all the features with nulls\n",
    "def count_nulls(df):\n",
    "    null_dict = {}\n",
    "    for i in df.columns:\n",
    "        if df[i].isnull().values.any():\n",
    "            null_dict[i]=df[i].isnull().sum()\n",
    "    return sorted(null_dict.items(), key=lambda x: x[1])\n",
    "# Code reuse from: https://stackoverflow.com/questions/29530232/how-to-check-if-any-value-is-nan-in-a-pandas-dataframe, https://www.codingem.com/python-sort-dictionary/,\n",
    "\n",
    "count_nulls(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = full_df.dropna(subset=['bud_wet_weight', 'grow_room_size', 'num_of_plants', 'light_watt'])\n",
    "count_nulls(full_df), full_df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other features but gremination can be filled with the median, different category or any rational way that fits the each feature. For the germination I will look for the unique values and try to figure it out from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df['germination'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = full_df.drop(\"germination\", axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I see it, we have more null values than unique values so I'll drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_feat = dict(count_nulls(full_df)).keys()\n",
    "null_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code reuse: https://stackoverflow.com/questions/58994588/how-can-i-add-the-counts-to-the-histogram-plot\n",
    "def get_feature_hist(df, cols, figsize=(10,10), rotation=0):\n",
    "    fig, ax = plt.subplots(1, len(cols))\n",
    "    for i, col in enumerate(cols):\n",
    "        if len(cols) > 1:\n",
    "            axis = ax[i]\n",
    "        else:\n",
    "            axis = ax\n",
    "        col_vals = full_df[col].value_counts()\n",
    "        bins = range(len(col_vals)+1)\n",
    "        ticks = range(len(col_vals))\n",
    "        labelist =[x[:15]+ \"...\" if len(x) > 15 else x for x in col_vals.keys()]\n",
    "        df[col].hist(grid=False, bins=bins, align='left', figsize=figsize, xrot=rotation)\n",
    "        axis.set_xticks(ticks, labelist, size='small')\n",
    "        for b, n in enumerate(col_vals):\n",
    "            axis.text(b-(0.1*len(str(n)))/2,  n+15, str(int(n)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_feature_hist(full_df, ['watering'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling with the forward fill\n",
    "full_df['watering'] = full_df['watering'].fillna(method='ffill')\n",
    "get_feature_hist(full_df,['watering'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_nulls(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def before_after_fill(df, col, method, val = None):\n",
    "    \"\"\"\n",
    "    Shows the differences in the values apperance before and after filling the null values\n",
    "    return: the feature without nulld\n",
    "    \"\"\"\n",
    "    print(\"Before:\\n\",df[col].value_counts()[:10])\n",
    "    if method == 'ffill':\n",
    "        new_col =df[col].fillna(method=method)\n",
    "    elif method == 'zeros':\n",
    "        new_col = df[col].fillna(0)\n",
    "    elif method == 'value' and val:\n",
    "        new_col = df[col].fillna(val)\n",
    "\n",
    "    print(\"\\nAfter:\\n\", new_col.value_counts()[:10])\n",
    "    return new_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling with the forward fill\n",
    "full_df['nutrients'] = before_after_fill(full_df,['nutrients'], 'ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values were filled using the most frequent value\n",
    "full_df['soil'] = before_after_fill(full_df,['soil'], 'value', full_df['soil'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A grower might not apply any grow technique so the null values will be replaced by zeros\n",
    "full_df['grow_techniques'] = before_after_fill(full_df,['grow_techniques'], 'zeros')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.isna().all()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, there are no null values in the dataset, now it is ready to convert it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_df.shape)\n",
    "full_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = full_df['strains_company'].value_counts().count()\n",
    "full_df['strains_company'].hist(figsize=(30,15), xrot=-90, edgecolor='darkcyan', bins=bins, grid=False, align='left', color='lightblue')\n",
    "plt.plot([-0.5, bins], [10, 10], color=\"black\", lw=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "companies_list = full_df['strains_company'].value_counts()[full_df['strains_company'].value_counts() > 10]\n",
    "companies_dict = {comp: i+2 for i, comp in enumerate(reversed(companies_list.keys()))}\n",
    "for comp in full_df['strains_company'].value_counts()[full_df['strains_company'].value_counts() <= 10].keys():\n",
    "    companies_dict[comp] = 1\n",
    "full_df['strains_company'] =full_df['strains_company'].map(companies_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = full_df['strains_company'].value_counts().count()\n",
    "full_df['strains_company'].hist(figsize=(15,10), xrot=-90, edgecolor='darkcyan', bins=bins, grid=False, align='left', color='lightblue')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADD BOLD TO THE NUMBERS HERE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of strains in the data 1872\n",
      "\n",
      "The number of strains with 5 or more appearances in the data 147\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of strains in the data\", full_df['strain'].value_counts().count())\n",
    "print(\"\\nThe number of strains with 5 or more appearances in the data\",full_df['strain'].value_counts()[full_df['strain'].value_counts() > 5].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strain_list = full_df['strain'].value_counts()[full_df['strain'].value_counts() > 5]\n",
    "strain_dict = {comp: i+2 for i, comp in enumerate(reversed(strain_list.keys()))}\n",
    "for strain in full_df['strain'].value_counts()[full_df['strain'].value_counts() <= 5].keys():\n",
    "    strain_dict[strain] = 1\n",
    "full_df['strain'] =full_df['strain'].map(strain_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wat_list = []\n",
    "for wat in full_df['light_watt']:\n",
    "    power = wat.replace(\" watt\", \"\")\n",
    "    if power.isnumeric():\n",
    "        wat_list.append(power)\n",
    "    else:\n",
    "        power1 = re.findall(\"[1-9]{1}[0-9]+W\", power)\n",
    "        if not power1:\n",
    "            power1 = re.findall(\"[1-9]{1}[0-9]+\", power)\n",
    "        wat_list.append(power1)\n",
    "        \n",
    "full_df.insert(4, \"light_new\", wat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrients_len_list = []\n",
    "# count the number of nutrients used\n",
    "for nut in full_df['nutrients']:\n",
    "    nut_count = len(nut.split(\",\"))-1\n",
    "    nutrients_len_list.append(nut_count)\n",
    "\n",
    "full_df.insert(6, \"nutrients_count\", nutrients_len_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "watering_dict = {watering: i for i, watering in enumerate(reversed(full_df['watering'].value_counts().keys()))}\n",
    "full_df['watering'] =full_df['watering'].map(watering_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataScienceIntro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
