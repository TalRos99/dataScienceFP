{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "bold_start = '\\033[1m'\n",
    "underline_start = '\\033[4m'\n",
    "bu_end = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def click_btn(clickable):\n",
    "    for btn in clickable:\n",
    "        try:\n",
    "            btn.click()\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project data\n",
    "The entire dataset I'm using in this project is scraped from https://growdiaries.com/, as one of the significant weed-growing communities in the world, it holds a large amount of data. Each row represents a data point, each column represents one of the diary features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1:\n",
    "How to get the details from all these diaries? <br>\n",
    "I started with a naive approach, trying to load 10K of diaries using the website UI wasn't successful. So I looked deeply into the query made each time more diaries are added. Using Postman (API requests tool), I realized that the query has a limit of 2K diaries and a starting point and count of how many rows the server should retrieve to the user. Adding on top of it is the fact that I can always be blocked from the website side.\n",
    "With those conclusions I decided to download the entire HTML pages that the query returns with 2K chunks, using it I scraped 10K links of grow diaries into my first database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# startpoint = 0\n",
    "# count = 2000\n",
    "# prefix_url = \"https://growdiaries.com/explore\"\n",
    "\n",
    "# service = Service('./chromedriver.exe')\n",
    "# op = webdriver.ChromeOptions()\n",
    "# op.add_argument('--headless')\n",
    "# elements = []\n",
    "# url_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver = webdriver.Chrome(service=service, options=op)\n",
    "# while startpoint < 10000:\n",
    "#     query = f\"?action=loadpage&category=all&tags=harvested&start={startpoint}&count={count}\"\n",
    "#     final_url = prefix_url + query\n",
    "#     driver.get(final_url)\n",
    "#     elements= driver.find_elements(By.XPATH, \"//a[@class='name']\")\n",
    "#     startpoint += count\n",
    "#     for element in elements:\n",
    "#         temp_df = pd.DataFrame({\"DiaryName\": [element.text], \"Url\": [element.get_attribute('href')]})\n",
    "#         url_df = pd.concat([url_df, temp_df], ignore_index=True)\n",
    "\n",
    "# driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url_df.to_csv(\"Diary Links.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2:\n",
    "Now, when having the links for my desired database, I need to start collecting the raw data from each diary. As the first action toward the real data, my decision was to include the following as features:\n",
    "diary_name<br> strain<br> strains_company<br> light_watt<br> nutrients<br> watering<br> soil<br> germination<br> grow_techniques<br>grow_room_size<br> weeks_to_harvest<br> num_of_plants<br> likes<br>comments<br> views<br> bud_dry_weight<br> bud_wet_weight\n",
    "\n",
    "Using likes, comments, and views may help to increase/decrease the reliability of the data point.<br>\n",
    "The method used to scrape the data is XPATH, it have conditional filtering and I found it as the perfect way to reach every selector I needed.\n",
    "\n",
    "Highly recommand on: https://devhints.io/xpath for understanding and creating xpath queries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note: To make it esaier, I saved each time 1K of data in a file</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_df = pd.read_csv(\"Diary Links.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver = webdriver.Chrome(service=service, options=op)\n",
    "# count = 2000\n",
    "# const = 1000\n",
    "# while count < 10000:\n",
    "#     det_df = pd.DataFrame()\n",
    "#     for j, url in enumerate(url_df[\"Url\"][count:count+const]):\n",
    "#         driver.get(url)\n",
    "#         driver.maximize_window()\n",
    "#         clickable = driver.find_elements(By.XPATH, \"//div[@class='btn_less']\")\n",
    "#         click_btn(clickable)\n",
    "#         report_items = driver.find_elements(By.XPATH, \"//div[contains(@class,'report_items')]//*[@class='info']\")\n",
    "#         outcome = driver.find_elements(By.XPATH, \"//div[contains(@class,'parameters_item')]\")\n",
    "#         likes = driver.find_element(By.XPATH, \"//div[@class='report_statistic']//div[.//@class='icon-leaf-like']\")\n",
    "#         comments = driver.find_element(By.XPATH, \"//div[@class='report_statistic']//div[.//@class='icon comment']\")\n",
    "#         views = driver.find_element(By.XPATH, \"//div[@class='report_statistic']//div[.//@class='icon eye']\")\n",
    "#         d = {\"diary_name\": url_df['DiaryName'].loc[count+j], \"strain\": \"\", \"strains_company\":\"\", \"light_watt\": \"\", \"nutrients\": \"\", \"watering\": \"\", \"soil\": \"\", \"germination\": \"\", 'grow_techniques': \"\",\n",
    "#         \"weeks_to_harvest\": \"\", \"num_of_plants\": \"\",\"likes\": likes.text, \"comments\": comments.text,\"views\": views.text,\"bud_dry_weight\": \"\", \"bud_wet_weight\": \"\"}\n",
    "\n",
    "        # for i, item in enumerate(report_items):\n",
    "        #     try:\n",
    "        #         text = item.text\n",
    "        #         texts = text.split(\"\\n\")\n",
    "        #         if i == 0:\n",
    "        #             d[\"strain\"] = texts[0]\n",
    "        #             d['strains_company'] = texts[1]\n",
    "        #         elif \"LED\" in text:\n",
    "        #             d['light_watt'] += text.replace(\"\\n\", \" \") + \",\"\n",
    "        #         elif \"Nutrients\" in text:\n",
    "        #             d['nutrients'] += texts[0] +\",\"\n",
    "        #         elif \"Watering\" in text:\n",
    "        #             d['watering'] = texts[0]\n",
    "        #         elif \"Soil\" in text or \"Grow\" in text:\n",
    "        #             d['soil']+= text.replace(\"\\n\", \" \")+\",\"\n",
    "        #         elif \"Germination\" in text:\n",
    "        #             d['germination'] = texts[0]\n",
    "        #         elif text.find(\"Week\") > 0 and len(texts[0]) > 1:\n",
    "        #             d['grow_techniques'] += texts[0] + \",\"\n",
    "        #     except:\n",
    "        #         pass\n",
    "        # for i, details in enumerate(outcome):\n",
    "        #     try:\n",
    "        #         text = details.text\n",
    "        #         texts = text.split(\"\\n\")\n",
    "        #         if i == 0:\n",
    "        #             d[\"weeks_to_harvest\"] = texts[1]\n",
    "        #         elif \"BUD WET WEIGHT\" in text:\n",
    "        #             d['bud_wet_weight']= texts[1]\n",
    "        #         elif \"BUD DRY WEIGHT\" in text:\n",
    "        #             d['bud_dry_weight']= texts[1]\n",
    "        #         elif \"NUMBER OF PLANTS HARVESTED\" in text:\n",
    "        #             d['num_of_plants'] = texts[1].split(\" \")[0]\n",
    "        #         elif \"TOTAL LIGHT POWER\" in text:\n",
    "        #             d['light_watt'] = texts[1]\n",
    "        #     except:\n",
    "        #         pass\n",
    "        # det_df = pd.concat([det_df,pd.DataFrame(d, index=[0])], ignore_index=True)\n",
    "    # det_df.to_csv(f\"Data_{count}_{count+const}.csv\", index=False)\n",
    "    # count += const\n",
    "\n",
    "# driver.quit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connecting all files to one csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# const = 1000\n",
    "# full_df = pd.DataFrame()\n",
    "# while count < 10000:\n",
    "#     full_df = pd.concat([full_df, pd.read_csv(f\"Data_{count}_{count+const}.csv\")])\n",
    "#     count += const\n",
    "# full_df.to_csv(\"GrowDiariesRowData.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial analysis and data refinement: \n",
    "<small>Nulls: np.nan/NaN/None/NA</small><br>\n",
    "Strating with testing which one of the dry/wet bud weight have less null values and set it as the target variable. <br>\n",
    "After, I was looking for the number of null values in each feature nd wrote it into a dictionery, that's the way to find out the features contains null and how many."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.read_csv(\"GrowDiariesRowData1.csv\")\n",
    "full_df['bud_dry_weight'].isna().sum(), full_df['bud_wet_weight'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = full_df.drop(\"bud_dry_weight\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing all the features with nulls\n",
    "def count_nulls(df):\n",
    "    null_dict = {}\n",
    "    for i in df.columns:\n",
    "        if df[i].isnull().values.any():\n",
    "            null_dict[i]=df[i].isnull().sum()\n",
    "    return sorted(null_dict.items(), key=lambda x: x[1])\n",
    "# Code reuse from: https://stackoverflow.com/questions/29530232/how-to-check-if-any-value-is-nan-in-a-pandas-dataframe, https://www.codingem.com/python-sort-dictionary/,\n",
    "\n",
    "count_nulls(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = full_df.dropna(subset=['bud_wet_weight', 'num_of_plants', 'light_watt'])\n",
    "count_nulls(full_df), full_df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other features but gremination can be filled with the median, different category or any rational way that fits the each feature. For the germination I will look for the unique values and try to figure it out from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df['germination'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = full_df.drop(\"germination\", axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I see it, we have more null values than unique values so I'll drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_feat = dict(count_nulls(full_df)).keys()\n",
    "null_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code reuse: https://stackoverflow.com/questions/58994588/how-can-i-add-the-counts-to-the-histogram-plot\n",
    "def get_feature_hist(df, cols, figsize=(10,10), rotation=0):\n",
    "    fig, ax = plt.subplots(1, len(cols))\n",
    "    for i, col in enumerate(cols):\n",
    "        if len(cols) > 1:\n",
    "            axis = ax[i]\n",
    "        else:\n",
    "            axis = ax\n",
    "        col_vals = full_df[col].value_counts()\n",
    "        bins = range(len(col_vals)+1)\n",
    "        ticks = range(len(col_vals))\n",
    "        labelist =[x[:15]+ \"...\" if len(x) > 15 else x for x in col_vals.keys()]\n",
    "        df[col].hist(grid=False, bins=bins, align='left', figsize=figsize, xrot=rotation)\n",
    "        axis.set_xticks(ticks, labelist, size='small')\n",
    "        for b, n in enumerate(col_vals):\n",
    "            axis.text(b-(0.1*len(str(n)))/2,  n+15, str(int(n)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_feature_hist(full_df, ['watering'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling with the forward fill\n",
    "full_df['watering'] = full_df['watering'].fillna(method='ffill')\n",
    "get_feature_hist(full_df,['watering'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_nulls(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def before_after_fill(df, col, method, val = None):\n",
    "    \"\"\"\n",
    "    Shows the differences in the values apperance before and after filling the null values\n",
    "    return: the feature without nulld\n",
    "    \"\"\"\n",
    "    print(\"Before:\\n\",df[col].value_counts()[:10])\n",
    "    if method == 'ffill':\n",
    "        new_col =df[col].fillna(method=method)\n",
    "    elif method == 'zeros':\n",
    "        new_col = df[col].fillna(0)\n",
    "    elif method == 'value' and val:\n",
    "        new_col = df[col].fillna(val)\n",
    "\n",
    "    print(\"\\nAfter:\\n\", new_col.value_counts()[:10])\n",
    "    return new_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling with the forward fill\n",
    "full_df['nutrients'] = before_after_fill(full_df,['nutrients'], 'ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values were filled using the most frequent value\n",
    "full_df['soil'] = before_after_fill(full_df,['soil'], 'value', full_df['soil'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A grower might not apply any grow technique so the null values will be replaced by zeros\n",
    "full_df['grow_techniques'] = before_after_fill(full_df,['grow_techniques'], 'zeros')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.isna().all()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, there are no null values in the dataset, now it is ready to convert it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_df.shape)\n",
    "full_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The line shows the threshold (10) \n",
    "bins = full_df['strains_company'].value_counts().count()\n",
    "full_df['strains_company'].hist(figsize=(30,15), xrot=-90, edgecolor='darkcyan', bins=bins, grid=False, align='left', color='lightblue')\n",
    "plt.plot([-0.5, bins], [10, 10], color=\"black\", lw=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the company appears more than 10 times it gets a unique value, otherwise it gets 1\n",
    "companies_list = full_df['strains_company'].value_counts()[full_df['strains_company'].value_counts() > 10]\n",
    "companies_dict = {comp: i+2 for i, comp in enumerate(reversed(companies_list.keys()))}\n",
    "for comp in full_df['strains_company'].value_counts()[full_df['strains_company'].value_counts() <= 10].keys():\n",
    "    companies_dict[comp] = 1\n",
    "full_df['strains_company'] =full_df['strains_company'].map(companies_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = full_df['strains_company'].value_counts().count()\n",
    "full_df['strains_company'].hist(figsize=(15,10), xrot=-90, edgecolor='darkcyan', bins=bins, grid=False, align='left', color='lightblue')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The number of strains in the data \" +bold_start +underline_start + str(full_df['strain'].value_counts().count())+ bu_end)\n",
    "print(\"\\nThe number of strains with 5 or more appearances in the data \"  +bold_start +underline_start + str(full_df['strain'].value_counts()[full_df['strain'].value_counts() > 5].count()) + bu_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strain_list = full_df['strain'].value_counts()[full_df['strain'].value_counts() > 5]\n",
    "strain_dict = {comp: i+2 for i, comp in enumerate(reversed(strain_list.keys()))}\n",
    "for strain in full_df['strain'].value_counts()[full_df['strain'].value_counts() <= 5].keys():\n",
    "    strain_dict[strain] = 1\n",
    "full_df['strain'] =full_df['strain'].map(strain_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "light_wat_int = []\n",
    "for e in full_df['light_watt']:\n",
    "    if \"watt\" not in e:\n",
    "        t = re.findall(\"[0-9]{2,}[W|X]\", e)\n",
    "        if not t:\n",
    "            light_wat_int.append(np.nan)\n",
    "        else:\n",
    "            light_wat_int.append(int(t[0][:-1]))\n",
    "    else:\n",
    "        light_wat_int.append(int(e.split(\" \")[0]))\n",
    "        \n",
    "full_df.insert(4, \"light_new\", light_wat_int)\n",
    "full_df = full_df.drop('light_watt', axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vals(df, col):\n",
    "    count_lst = []\n",
    "    # count the number of nutrients used\n",
    "    for val in full_df[col]:\n",
    "        try:\n",
    "            val_count = len(val.split(\",\"))-1\n",
    "        except:\n",
    "            val_count = val\n",
    "        count_lst.append(val_count)\n",
    "\n",
    "    return count_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrients_len_list = count_vals(full_df, 'nutrients')\n",
    "\n",
    "full_df.insert(6, \"nutrients_count\", nutrients_len_list)\n",
    "full_df = full_df.drop('nutrients', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping the watering methods to numbers\n",
    "watering_dict = {watering: i for i, watering in enumerate(reversed(full_df['watering'].value_counts().keys()))}\n",
    "full_df['watering'] =full_df['watering'].map(watering_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soil_lst = count_vals(full_df, 'soil')\n",
    "\n",
    "full_df.insert(9, \"soils_count\", soil_lst)\n",
    "full_df = full_df.drop('soil', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grow_lst = count_vals(full_df, 'grow_techniques')\n",
    "\n",
    "full_df.insert(11, \"grow_techniques_count\", grow_lst)\n",
    "full_df = full_df.drop('grow_techniques', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = full_df.dropna().drop_duplicates()\n",
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to int\n",
    "full_df['weeks_to_harvest'] = [int(t) for t in full_df['weeks_to_harvest']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation grid\n",
    "sns.heatmap(full_df.drop(\"diary_name\", axis=1).corr(), cmap='Greys')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The correlation between likes and comments is quite obvious, and later I will use PCA to create new orthogonal basis vectors for it, in order to assess whether user engagement could lend credibility to a diary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.hist(figsize=(12,12), grid=False)\n",
    "print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_outliers(df, lst, figtitle):\n",
    "    fig, ax = plt.subplots(1, 7, figsize=(20,8))\n",
    "    i = 0\n",
    "    fig.suptitle(figtitle, fontsize=16)\n",
    "    for c in lst:\n",
    "        ax[i].boxplot(df[c])\n",
    "        ax[i].set_title(c)\n",
    "        i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_outliers(full_df, ['light_new', 'nutrients_count', 'weeks_to_harvest', 'likes', 'comments', 'views', 'bud_wet_weight'], 'Before outliers remove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_detection_iqr(df):\n",
    "    cdf = df.copy(deep=True)\n",
    "    for col in cdf.columns:\n",
    "        if cdf[col].dtype in ['float64', 'int64'] and col in ['light_new', 'nutrients_count', 'weeks_to_harvest', 'likes', 'comments', 'views', 'bud_wet_weight']:\n",
    "            Q1 = cdf[col].quantile(0.25)\n",
    "            Q3 = cdf[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            cdf.loc[(cdf[col] < (Q1 - 1.5*IQR)), col] = np.nan\n",
    "            cdf.loc[(cdf[col] > (Q3 + 1.5*IQR)), col] = np.nan\n",
    "\n",
    "    return cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = outlier_detection_iqr(full_df).dropna()\n",
    "plot_outliers(full_df, ['light_new', 'nutrients_count', 'weeks_to_harvest', 'likes', 'comments', 'views', 'bud_wet_weight'], \"After outliers removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df[full_df['bud_wet_weight'] > 0.5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Following the previous question about user engagement, now I'm trying to find out if there is a direct correlation between the number of finances and time invested in a diary (for example, if all highly engaged diaries tend to contain plenty of nutrients kinds?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(12,8))\n",
    "sns.scatterplot(data=full_df, x = \"likes\", y='comments', size='nutrients_count',hue='grow_techniques_count', ax=ax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataScienceIntro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
