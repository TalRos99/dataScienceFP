{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def click_btn(clickable):\n",
    "    for btn in clickable:\n",
    "        try:\n",
    "            btn.click()\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# startpoint = 0\n",
    "# count = 1000\n",
    "# prefix_url = \"https://growdiaries.com/explore\"\n",
    "\n",
    "service = Service('./chromedriver.exe')\n",
    "op = webdriver.ChromeOptions()\n",
    "op.add_argument('--headless')\n",
    "# elements = []\n",
    "# url_df = pd.DataFrame(data={\"DiaryName\": \"\", \"Url\": \"\"})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "אחרי שניסיתי לטעון בדפדפן המון אופציות, השתמשתי בפוסטמן בשביל לבדוק את הפילטר של בקשת האיפיאי וראיתי שאני יכול למשוך קובץ של 2000 לינקים ליומנים\n",
    "שמרתי את הדפי ווב של הקישורים ואז יצרתי דאטהבייס של קישורים ללינקים."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver = webdriver.Chrome(service=service, options=op)\n",
    "# while startpoint < 10000:\n",
    "#     query = f\"?action=loadpage&category=all&tags=harvested&start={startpoint}&count={count}&undefined=&ajax=true&ust=9c824c5a63187dd5f579502e26ac3594c3dbb8752fd954432a4d6e1b0ff2e41f8654e9f3\"\n",
    "#     final_url = prefix_url + query\n",
    "#     driver.get(final_url)\n",
    "#     elements= driver.find_elements(By.XPATH, \"//a[@class='name']\")\n",
    "#     startpoint += count\n",
    "#     for element in elements:\n",
    "#         temp_df = pd.DataFrame({\"DiaryName\": [element.text], \"Url\": [element.get_attribute('href')]})\n",
    "#         url_df = pd.concat([url_df, temp_df], ignore_index=True)\n",
    "# driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url_df.to_csv(\"Diary Links.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_df = pd.read_csv(\"Diary Links.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://growdiaries.com/diaries/154116-grow-journal-by-yan607'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_df['Url'].loc[2095]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "כעת אני עובר על כל אחד מהקישורים ושומר את המידע בצ'אנקים של 1000 רשומות"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(service=service, options=op)\n",
    "count = 2000\n",
    "const = 1000\n",
    "while count < 10000:\n",
    "    det_df = pd.DataFrame()\n",
    "    for j, url in enumerate(url_df[\"Url\"][count:count+const]):\n",
    "        driver.get(url)\n",
    "        driver.maximize_window()\n",
    "        clickable = driver.find_elements(By.XPATH, \"//div[@class='btn_less']\")\n",
    "        click_btn(clickable)\n",
    "        report_items = driver.find_elements(By.XPATH, \"//div[contains(@class,'report_items')]//*[@class='info']\")\n",
    "        outcome = driver.find_elements(By.XPATH, \"//div[contains(@class,'parameters_item')]\")\n",
    "        likes = driver.find_element(By.XPATH, \"//div[@class='report_statistic']//div[.//@class='icon-leaf-like']\")\n",
    "        # add comments as well!!!\n",
    "        d = {\"diary_name\": url_df['DiaryName'].loc[count+j], \"strain\": \"\", \"strains_company\":\"\", \"light_watt\": \"\", \"nutrients\": \"\", \"indoor\": \"\", \"watering\": \"\", \"soil\": \"\", \"germination\": \"\", 'grow_techniques': \"\", \"grow_room_size\": \"\",\n",
    "    \"weeks_to_harvest\": \"\", \"num_of_plants\": \"\",likes: likes.text, \"bud_dry_weight\": \"\", \"bud_wet_weight\": \"\"}\n",
    "\n",
    "        for i, item in enumerate(report_items):\n",
    "            try:\n",
    "                text = item.text\n",
    "                texts = text.split(\"\\n\")\n",
    "                if i == 0:\n",
    "                    d[\"strain\"] = texts[0]\n",
    "                    d['strains_company'] = texts[1]\n",
    "                elif \"LED\" in text:\n",
    "                    d['light_watt'] += text.replace(\"\\n\", \" \") + \",\"\n",
    "                elif \"Nutrients\" in text:\n",
    "                    d['nutrients'] += texts[0] +\",\"\n",
    "                elif \"Room Type\" in text:\n",
    "                    d['indoor'] = True\n",
    "                elif \"Watering\" in text:\n",
    "                    d['watering'] = texts[0]\n",
    "                elif \"Soil\" in text or \"Grow\" in text:\n",
    "                    d['soil']+= text.replace(\"\\n\", \" \")+\",\"\n",
    "                elif \"Germination\" in text:\n",
    "                    d['germination'] = texts[0]\n",
    "                elif text.find(\"Week\") > 0 and len(texts[0]) > 1:\n",
    "                    d['grow_techniques'] += texts[0] + \",\"\n",
    "                elif text.find(\"Tent\") > 0:\n",
    "                    d['grow_room_size'] = texts[0]\n",
    "            except:\n",
    "                pass\n",
    "        for i, details in enumerate(outcome):\n",
    "            try:\n",
    "                text = details.text\n",
    "                texts = text.split(\"\\n\")\n",
    "                if i == 0:\n",
    "                    d[\"weeks_to_harvest\"] = texts[1]\n",
    "                elif \"BUD WET WEIGHT\" in text:\n",
    "                    d['bud_wet_weight']= texts[1]\n",
    "                elif \"BUD DRY WEIGHT\" in text:\n",
    "                    d['bud_dry_weight']= texts[1]\n",
    "                elif \"NUMBER OF PLANTS HARVESTED\" in text:\n",
    "                    d['num_of_plants'] = texts[1].split(\" \")[0]\n",
    "                elif \"TOTAL LIGHT POWER\" in text:\n",
    "                    d['light_watt'] = texts[1]\n",
    "                elif \"GROW ROOM\" in text:\n",
    "                    d['grow_room_size']= texts[1]\n",
    "            except:\n",
    "                pass\n",
    "        det_df = pd.concat([det_df,pd.DataFrame(d, index=[0])], ignore_index=True)\n",
    "    det_df.to_csv(f\"Data_{count}_{count+const}.csv\", index=False)\n",
    "    count += const\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(service=service, options=op)\n",
    "det_df = pd.DataFrame()\n",
    "d = []\n",
    "for j, url in enumerate(url_df[\"Url\"]):\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        driver.maximize_window()\n",
    "        likes = driver.find_element(By.XPATH, \"//div[@class='report_statistic']//div[.//@class='icon-leaf-like']\")\n",
    "        d.append(likes.text) \n",
    "    except:\n",
    "        d.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'50'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_df['likes'] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_df.to_csv(\"Diary links with likes.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://growdiaries.com/diaries/164797-grow-journal-by-piuswaxis'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_df['Url'].loc[209]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataScienceIntro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
