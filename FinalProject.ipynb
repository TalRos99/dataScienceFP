{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def click_btn(clickable):\n",
    "    for btn in clickable:\n",
    "        try:\n",
    "            btn.click()\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project data\n",
    "The entire dataset I'm using in this project is from https://growdiaries.com/, as one of the significant weed-growing communities in the world, it holds a large amount of data. Each row represents a data point, each column represents one of the diary features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1:\n",
    "How to get the details from all these diaries? \n",
    "I started with a naive approach, trying to load 10K of diaries using the website UI wasn't a great idea. So I took a deep look into the query of the API requests the website is making. Using Postman I realized that the query has a limit of 2K diaries and it has a starting point and count of how many rows should the server retrieve to the user. Adding on top of it is the fact that I can always be blocked from the website side.\n",
    "With those conclusions I decided to download the entire HTML pages that the query returns with 2K chunks, using it I scraped 10K links of grow diaries into my first database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# startpoint = 0\n",
    "# count = 2000\n",
    "# prefix_url = \"https://growdiaries.com/explore\"\n",
    "\n",
    "# service = Service('./chromedriver.exe')\n",
    "# op = webdriver.ChromeOptions()\n",
    "# # op.add_argument('--headless')\n",
    "# elements = []\n",
    "# url_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver = webdriver.Chrome(service=service, options=op)\n",
    "# while startpoint < 10000:\n",
    "#     query = f\"?action=loadpage&category=all&tags=harvested&start={startpoint}&count={count}&undefined=&ajax=true&ust=9c824c5a63187dd5f579502e26ac3594c3dbb8752fd954432a4d6e1b0ff2e41f8654e9f3\"\n",
    "#     final_url = prefix_url + query\n",
    "#     driver.get(final_url)\n",
    "#     elements= driver.find_elements(By.XPATH, \"//a[@class='name']\")\n",
    "#     startpoint += count\n",
    "#     for element in elements:\n",
    "#         temp_df = pd.DataFrame({\"DiaryName\": [element.text], \"Url\": [element.get_attribute('href')]})\n",
    "#         url_df = pd.concat([url_df, temp_df], ignore_index=True)\n",
    "    \n",
    "# driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url_df.to_csv(\"Diary Links.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2:\n",
    "Now, when having the links for my desired database, I need to start collecting the raw data from each diary. As the first action toward the real data, my decision was to include the following as features:\n",
    "diary_name<br> strain<br> strains_company<br> light_watt<br> nutrients<br> watering<br> soil<br> germination<br> grow_techniques<br>grow_room_size<br> weeks_to_harvest<br> num_of_plants<br> likes<br>comments<br> views<br> bud_dry_weight<br> bud_wet_weight\n",
    "\n",
    "Using likes, comments, and views may help to increase/decrease the reliability of the data point.<br>\n",
    "The method used to scrape the data is XPATH, it have conditional filtering and I found it as the perfect way to reach every selector I needed.\n",
    "\n",
    "Highly recommand on: https://devhints.io/xpath for understanding and creating xpath queries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note: To make it esaier, I saved each time 1K of data in a file</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_df = pd.read_csv(\"Diary Links.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver = webdriver.Chrome(service=service, options=op)\n",
    "# count = 2000\n",
    "# const = 1000\n",
    "# while count < 10000:\n",
    "#     det_df = pd.DataFrame()\n",
    "#     for j, url in enumerate(url_df[\"Url\"][count:count+const]):\n",
    "#         driver.get(url)\n",
    "#         driver.maximize_window()\n",
    "#         clickable = driver.find_elements(By.XPATH, \"//div[@class='btn_less']\")\n",
    "#         click_btn(clickable)\n",
    "#         report_items = driver.find_elements(By.XPATH, \"//div[contains(@class,'report_items')]//*[@class='info']\")\n",
    "#         outcome = driver.find_elements(By.XPATH, \"//div[contains(@class,'parameters_item')]\")\n",
    "#         likes = driver.find_element(By.XPATH, \"//div[@class='report_statistic']//div[.//@class='icon-leaf-like']\")\n",
    "#         comments = driver.find_element(By.XPATH, \"//div[@class='report_statistic']//div[.//@class='icon comment']\")\n",
    "#         views = driver.find_element(By.XPATH, \"//div[@class='report_statistic']//div[.//@class='icon eye']\")\n",
    "#         d = {\"diary_name\": url_df['DiaryName'].loc[count+j], \"strain\": \"\", \"strains_company\":\"\", \"light_watt\": \"\", \"nutrients\": \"\", \"watering\": \"\", \"soil\": \"\", \"germination\": \"\", 'grow_techniques': \"\", \"grow_room_size\": \"\",\n",
    "#         \"weeks_to_harvest\": \"\", \"num_of_plants\": \"\",\"likes\": likes.text, \"comments\": comments.text,\"views\": views.text,\"bud_dry_weight\": \"\", \"bud_wet_weight\": \"\"}\n",
    "\n",
    "        # for i, item in enumerate(report_items):\n",
    "        #     try:\n",
    "        #         text = item.text\n",
    "        #         texts = text.split(\"\\n\")\n",
    "        #         if i == 0:\n",
    "        #             d[\"strain\"] = texts[0]\n",
    "        #             d['strains_company'] = texts[1]\n",
    "        #         elif \"LED\" in text:\n",
    "        #             d['light_watt'] += text.replace(\"\\n\", \" \") + \",\"\n",
    "        #         elif \"Nutrients\" in text:\n",
    "        #             d['nutrients'] += texts[0] +\",\"\n",
    "        #         elif \"Watering\" in text:\n",
    "        #             d['watering'] = texts[0]\n",
    "        #         elif \"Soil\" in text or \"Grow\" in text:\n",
    "        #             d['soil']+= text.replace(\"\\n\", \" \")+\",\"\n",
    "        #         elif \"Germination\" in text:\n",
    "        #             d['germination'] = texts[0]\n",
    "        #         elif text.find(\"Week\") > 0 and len(texts[0]) > 1:\n",
    "        #             d['grow_techniques'] += texts[0] + \",\"\n",
    "        #         elif text.find(\"Tent\") > 0:\n",
    "        #             d['grow_room_size'] = texts[0]\n",
    "        #     except:\n",
    "        #         pass\n",
    "        # for i, details in enumerate(outcome):\n",
    "        #     try:\n",
    "        #         text = details.text\n",
    "        #         texts = text.split(\"\\n\")\n",
    "        #         if i == 0:\n",
    "        #             d[\"weeks_to_harvest\"] = texts[1]\n",
    "        #         elif \"BUD WET WEIGHT\" in text:\n",
    "        #             d['bud_wet_weight']= texts[1]\n",
    "        #         elif \"BUD DRY WEIGHT\" in text:\n",
    "        #             d['bud_dry_weight']= texts[1]\n",
    "        #         elif \"NUMBER OF PLANTS HARVESTED\" in text:\n",
    "        #             d['num_of_plants'] = texts[1].split(\" \")[0]\n",
    "        #         elif \"TOTAL LIGHT POWER\" in text:\n",
    "        #             d['light_watt'] = texts[1]\n",
    "        #         elif \"GROW ROOM\" in text:\n",
    "        #             d['grow_room_size']= texts[1]\n",
    "        #     except:\n",
    "        #         pass\n",
    "        # det_df = pd.concat([det_df,pd.DataFrame(d, index=[0])], ignore_index=True)\n",
    "    # det_df.to_csv(f\"Data_{count}_{count+const}.csv\", index=False)\n",
    "    # count += const\n",
    "\n",
    "# driver.quit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connecting all files to one csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# const = 1000\n",
    "# full_df = pd.DataFrame()\n",
    "# while count < 10000:\n",
    "#     full_df = pd.concat([full_df, pd.read_csv(f\"Data_{count}_{count+const}.csv\")])\n",
    "#     count += const\n",
    "# full_df.to_csv(\"GrowDiariesRowData.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial analysis and data refinement: \n",
    "<small>Nulls: np.nan/NaN/None/NA</small><br>\n",
    "Strating with testing which one of the dry/wet bud weight have less null values and set it as the target variable. <br>\n",
    "After, I was looking for the number of null values in each feature nd wrote it into a dictionery, that's the way to find out the features contains null and how many."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5676, 4091)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df = pd.read_csv(\"GrowDiariesRowData.csv\")\n",
    "full_df['bud_dry_weight'].isna().sum(), full_df['bud_wet_weight'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = full_df.drop(\"bud_dry_weight\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('diary_name', 1),\n",
       " ('strain', 2),\n",
       " ('strains_company', 2),\n",
       " ('weeks_to_harvest', 3),\n",
       " ('watering', 489),\n",
       " ('soil', 858),\n",
       " ('light_watt', 1462),\n",
       " ('num_of_plants', 1621),\n",
       " ('nutrients', 2074),\n",
       " ('grow_techniques', 2397),\n",
       " ('grow_room_size', 3675),\n",
       " ('germination', 3995),\n",
       " ('bud_wet_weight', 4091)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Showing all the features with nulls\n",
    "def count_nulls(df):\n",
    "    null_dict = {}\n",
    "    for i in df.columns:\n",
    "        if df[i].isnull().values.any():\n",
    "            null_dict[i]=df[i].isnull().sum()\n",
    "    return sorted(null_dict.items(), key=lambda x: x[1])\n",
    "# Code partly from: https://stackoverflow.com/questions/29530232/how-to-check-if-any-value-is-nan-in-a-pandas-dataframe, https://www.codingem.com/python-sort-dictionary/,\n",
    "\n",
    "count_nulls(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('watering', 117),\n",
       "  ('soil', 252),\n",
       "  ('nutrients', 560),\n",
       "  ('grow_techniques', 864),\n",
       "  ('germination', 1474)],\n",
       " (4218, 16))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df = full_df.dropna(subset=['bud_wet_weight', 'grow_room_size', 'num_of_plants', 'light_watt'])\n",
    "count_nulls(full_df), full_df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other features but gremination can be filled with the median, different category or any rational way that fits the each feature. For the germination I will look for the unique values and try to figure it out from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "germination\n",
       "Paper Towel              1001\n",
       "Glass Of Water            647\n",
       "Directly In Substrate     542\n",
       "Other                     255\n",
       "Peat Pellet               217\n",
       "Rockwool Cube              82\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df['germination'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = full_df.drop(\"germination\", axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I see it, we have more null values than unique values so I'll drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['soil', 'nutrients', 'grow_techniques'])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_feat = dict(count_nulls(full_df)).keys()\n",
    "null_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "watering\n",
       "Manual                                3703\n",
       "Hydroponics                            325\n",
       "Drip                                   157\n",
       "Aeroponics                              32\n",
       "Watering with Dirty Aquarium Water       1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df['watering'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "watering\n",
       "Manual                                3703\n",
       "Hydroponics                            325\n",
       "Drip                                   157\n",
       "Aeroponics                              32\n",
       "Watering with Dirty Aquarium Water       1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filling with the forward fill\n",
    "full_df['watering'] = full_df['watering'].fillna(method='ffill')\n",
    "full_df['watering'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('soil', 252), ('nutrients', 560), ('grow_techniques', 864)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_nulls(full_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three remain features can be count, I think the mapping of these features will be done in this way"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataScienceIntro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
