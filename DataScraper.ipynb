{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def click_btn(clickable):\n",
    "    for btn in clickable:\n",
    "        try:\n",
    "            btn.click()\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1:\n",
    "\n",
    "How to get the details from all these diaries? <br>\n",
    "I started with a naive approach. Trying to load 10K of diaries using the website UI wasn't successful. So I carefully examined the query that was sent when more diaries were added. Using Postman (API requests tool), I discovered that the query has a limit of 2K diaries and a starting point and count of how many rows the server should return. Added to it is the fact that I can always be blocked from the website side.\n",
    "\n",
    "With those conclusions I decided to download the entire HTML pages that the query returned with 2K chunks, using it I scraped 10K links of grow diaries into my first database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startpoint = 0\n",
    "count = 2000\n",
    "prefix_url = \"https://growdiaries.com/explore\"\n",
    "\n",
    "service = Service('./chromedriver.exe')\n",
    "op = webdriver.ChromeOptions()\n",
    "op.add_argument('--headless')\n",
    "elements = []\n",
    "url_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(service=service, options=op)\n",
    "while startpoint < 10000:\n",
    "    query = f\"?action=loadpage&category=all&tags=harvested&start={startpoint}&count={count}\"\n",
    "    final_url = prefix_url + query\n",
    "    driver.get(final_url)\n",
    "    elements= driver.find_elements(By.XPATH, \"//a[@class='name']\")\n",
    "    startpoint += count\n",
    "    for element in elements:\n",
    "        temp_df = pd.DataFrame({\"DiaryName\": [element.text], \"Url\": [element.get_attribute('href')]})\n",
    "        url_df = pd.concat([url_df, temp_df], ignore_index=True)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_df.to_csv(\"Diary Links.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2:\n",
    "\n",
    "Once I have the links for my desired database, I need to start collecting raw data from each diary. As the first step toward real data, my decision was to include the following as features: <br>\n",
    "diary_name<br> strain<br> strains_company<br> light_watt<br> nutrients<br> watering<br> soil<br> germination<br> grow_techniques<br>grow_room_size<br> weeks_to_harvest<br> num_of_plants<br> likes<br>comments<br> views<br> bud_dry_weight<br> bud_wet_weight\n",
    "\n",
    "Using likes, comments, and views may increase/decrease the reliability of the data point.<br>\n",
    "The method used to scrape the data is XPATH. It has conditional filtering and I found it the perfect way to reach every selector I needed.\n",
    "\n",
    "Highly recommend: https://devhints.io/xpath for understanding and creating XPath queries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note: To make it esaier, I saved each time 1K of data in a file</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_df = pd.read_csv(\"Diary Links.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(service=service, options=op)\n",
    "count = 2000\n",
    "const = 1000\n",
    "while count < 10000:\n",
    "    det_df = pd.DataFrame()\n",
    "    for j, url in enumerate(url_df[\"Url\"][count:count+const]):\n",
    "        driver.get(url)\n",
    "        driver.maximize_window()\n",
    "        clickable = driver.find_elements(By.XPATH, \"//div[@class='btn_less']\")\n",
    "        click_btn(clickable)\n",
    "        report_items = driver.find_elements(By.XPATH, \"//div[contains(@class,'report_items')]//*[@class='info']\")\n",
    "        outcome = driver.find_elements(By.XPATH, \"//div[contains(@class,'parameters_item')]\")\n",
    "        likes = driver.find_element(By.XPATH, \"//div[@class='report_statistic']//div[.//@class='icon-leaf-like']\")\n",
    "        comments = driver.find_element(By.XPATH, \"//div[@class='report_statistic']//div[.//@class='icon comment']\")\n",
    "        views = driver.find_element(By.XPATH, \"//div[@class='report_statistic']//div[.//@class='icon eye']\")\n",
    "        d = {\"diary_name\": url_df['DiaryName'].loc[count+j], \"strain\": \"\", \"strains_company\":\"\", \"light_watt\": \"\", \"nutrients\": \"\", \"watering\": \"\", \"soil\": \"\", \"germination\": \"\", 'grow_techniques': \"\",\n",
    "        \"weeks_to_harvest\": \"\", \"num_of_plants\": \"\",\"likes\": likes.text, \"comments\": comments.text,\"views\": views.text,\"bud_dry_weight\": \"\", \"bud_wet_weight\": \"\"}\n",
    "\n",
    "for i, item in enumerate(report_items):\n",
    "    try:\n",
    "        text = item.text\n",
    "        texts = text.split(\"\\n\")\n",
    "        if i == 0:\n",
    "            d[\"strain\"] = texts[0]\n",
    "            d['strains_company'] = texts[1]\n",
    "        elif \"LED\" in text:\n",
    "            d['light_watt'] += text.replace(\"\\n\", \" \") + \",\"\n",
    "        elif \"Nutrients\" in text:\n",
    "            d['nutrients'] += texts[0] +\",\"\n",
    "        elif \"Watering\" in text:\n",
    "            d['watering'] = texts[0]\n",
    "        elif \"Soil\" in text or \"Grow\" in text:\n",
    "            d['soil']+= text.replace(\"\\n\", \" \")+\",\"\n",
    "        elif \"Germination\" in text:\n",
    "            d['germination'] = texts[0]\n",
    "        elif text.find(\"Week\") > 0 and len(texts[0]) > 1:\n",
    "            d['grow_techniques'] += texts[0] + \",\"\n",
    "    except:\n",
    "        pass\n",
    "for i, details in enumerate(outcome):\n",
    "    try:\n",
    "        text = details.text\n",
    "        texts = text.split(\"\\n\")\n",
    "        if i == 0:\n",
    "            d[\"weeks_to_harvest\"] = texts[1]\n",
    "        elif \"BUD WET WEIGHT\" in text:\n",
    "            d['bud_wet_weight']= texts[1]\n",
    "        elif \"BUD DRY WEIGHT\" in text:\n",
    "            d['bud_dry_weight']= texts[1]\n",
    "        elif \"NUMBER OF PLANTS HARVESTED\" in text:\n",
    "            d['num_of_plants'] = texts[1].split(\" \")[0]\n",
    "        elif \"TOTAL LIGHT POWER\" in text:\n",
    "            d['light_watt'] = texts[1]\n",
    "    except:\n",
    "        pass\n",
    "det_df = pd.concat([det_df,pd.DataFrame(d, index=[0])], ignore_index=True)\n",
    "det_df.to_csv(f\"Data_{count}_{count+const}.csv\", index=False)\n",
    "count += const\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect all files to one CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "const = 1000\n",
    "full_df = pd.DataFrame()\n",
    "while count < 10000:\n",
    "    full_df = pd.concat([full_df, pd.read_csv(f\"Data_{count}_{count+const}.csv\")])\n",
    "    count += const\n",
    "full_df.to_csv(\"GrowDiariesRowData.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
