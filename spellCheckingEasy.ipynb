{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Project data\n",
      "The entire dataset I'm using in this project is scraped from https://growdiaries.com/, as one of the significant weed-growing communities in the world, it holds a large amount of data. Each row represents a data point, each column represents one of the diary features.\n",
      "\n",
      "### Step 1:\n",
      "How to get the details from all these diaries? <br>\n",
      "I started with a naive approach, trying to load 10K of diaries using the website UI wasn't successful. So I looked deeply into the query made each time more diaries are added. Using Postman (API requests tool), I realized that the query has a limit of 2K diaries and a starting point and count of how many rows the server should retrieve to the user. Adding on top of it is the fact that I can always be blocked from the website side.\n",
      "With those conclusions I decided to download the entire HTML pages that the query returns with 2K chunks, using it I scraped 10K links of grow diaries into my first database.\n",
      "\n",
      "### Step 2:\n",
      "Now, when having the links for my desired database, I need to start collecting the raw data from each diary. As the first action toward the real data, my decision was to include the following as features:\n",
      "diary_name<br> strain<br> strains_company<br> light_watt<br> nutrients<br> watering<br> soil<br> germination<br> grow_techniques<br>grow_room_size<br> weeks_to_harvest<br> num_of_plants<br> likes<br>comments<br> views<br> bud_dry_weight<br> bud_wet_weight\n",
      "Using likes, comments, and views may help to increase/decrease the reliability of the data point.<br>\n",
      "The method used to scrape the data is XPATH, it have conditional filtering and I found it as the perfect way to reach every selector I needed.\n",
      "Highly recommand on: https://devhints.io/xpath for understanding and creating xpath queries\n",
      "\n",
      "<b>Note: To make it esaier, I saved each time 1K of data in a file</b>\n",
      "\n",
      "Connecting all files to one csv file:\n",
      "\n",
      "### Initial analysis and data refinement:\n",
      "<small>Nulls: np.nan/NaN/None/NA</small><br>\n",
      "Strating with testing which one of the dry/wet bud weight have less null values and set it as the target variable. <br>\n",
      "After, I was looking for the number of null values in each feature nd wrote it into a dictionery, that's the way to find out the features contains null and how many.\n",
      "\n",
      "The other features but gremination can be filled with the median, different category or any rational way that fits the each feature. For the germination I will look for the unique values and try to figure it out from it.\n",
      "\n",
      "As I see it, we have more null values than unique values so I'll drop it.\n",
      "\n",
      "Now, there are no null values in the dataset, now it is ready to convert it\n",
      "\n",
      "## EDA & Visualization\n",
      "\n",
      "##### The correlation between likes and comments is quite obvious, and later I will use PCA to create new orthogonal basis vectors for it, in order to assess whether user engagement could lend credibility to a diary.\n",
      "\n",
      "### Outliers treatment\n",
      "\n",
      "##### Following the previous question about user engagement, now I'm trying to find out if there is a direct correlation between the number of finances and time invested in a diary (for example, if all highly engaged diaries tend to contain plenty of nutrients kinds?)\n",
      "\n",
      "#### Preforming PCA to likes and comments + Changing the indecies to diary_name\n",
      "In this point we are ready to preform the supervised machine learning.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def extract_source_from_markdown(json_file_path):\n",
    "    with open(json_file_path) as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    markdown_sources = []\n",
    "    for obj in data[\"cells\"]:\n",
    "        if obj.get(\"cell_type\") == \"markdown\":\n",
    "            source = obj.get(\"source\")\n",
    "            source = ''.join(source)\n",
    "            source = source.replace(\"\\n\\n\", \"\\n\")\n",
    "            if source:\n",
    "                markdown_sources.append(source)\n",
    "    \n",
    "    return markdown_sources\n",
    "\n",
    "file_path = \"FinalProject.ipynb\"\n",
    "markdown_sources = extract_source_from_markdown(file_path)\n",
    "\n",
    "for source in markdown_sources:\n",
    "    print(source)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataScienceIntro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
